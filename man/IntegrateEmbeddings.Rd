% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/integration.R
\name{IntegrateEmbeddings}
\alias{IntegrateEmbeddings}
\title{Integrate low dimensional embeddings}
\usage{
IntegrateEmbeddings(
  anchorset,
  new.reduction.name = NULL,
  reductions = NULL,
  dims.to.integrate = NULL,
  k.weight = 100,
  weight.reduction = NULL,
  sd.weight = 1,
  sample.tree = NULL,
  preserve.order = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{anchorset}{An \code{\link{AnchorSet}} object generated by
\code{\link{FindIntegrationAnchors}}}

\item{new.reduction.name}{Name for new integrated dimensional reduction.
Defaults to "integrated_" + name of first reduction.}

\item{reductions}{Name of reductions to be integrated. Can be either a single
string if reduction is present in all objects or a vector of strings, one for
each object.}

\item{dims.to.integrate}{Number of dimensions to return integrated values for}

\item{k.weight}{Number of neighbors to consider when weighting anchors}

\item{weight.reduction}{Dimension reduction to use when calculating anchor
weights. This can be one of:
\itemize{
   \item{A string, specifying the name of a dimension reduction present in
   all objects to be integrated}
   \item{A vector of strings, specifying the name of a dimension reduction to
   use for each object to be integrated}
   \item{A vector of \code{\link{DimReduc}} objects, specifying the object to
   use for each object in the integration}
   \item{NULL, in which case the full corrected space is used for computing
   anchor weights.}
}}

\item{sd.weight}{Controls the bandwidth of the Gaussian kernel for weighting}

\item{sample.tree}{Specify the order of integration. If NULL, will compute
automatically.}

\item{preserve.order}{Do not reorder objects based on size for each pairwise
integration.}

\item{verbose}{Print progress bars and output}
}
\description{
Perform dataset integration using a pre-computed Anchorset of specified low
dimensional representations.
}
\details{
The main steps of this procedure are identical to \code{\link{IntegrateData}}
with one key distinction. When computing the weights matrix, the distance
calculations are performed in the full space of integrated embeddings when
integrating more than two datasets, as opposed to a reduced PCA space which
is the default behavior in \code{\link{IntegrateData}}.
}
\examples{
\dontrun{

library(SeuratData)
data("panc8")

# panc8 is a merged Seurat object containing 8 separate pancreas datasets
# split the object by dataset
pancreas.list <- SplitObject(panc8, split.by = "tech")[1:2]

# perform standard preprocessing on each object
for (i in 1:length(pancreas.list)) {
 pancreas.list[[i]] <- NormalizeData(pancreas.list[[i]], verbose = FALSE)
 pancreas.list[[i]] <- FindVariableFeatures(
   pancreas.list[[i]], selection.method = "vst",
   nfeatures = 2000, verbose = FALSE
 )
 pancreas.list[[i]] <- ScaleData(pancreas.list[[i]], verbose = FALSE)
 pancreas.list[[i]] <- RunPCA(pancreas.list[[i]], verbose = FALSE)
}

# find anchors
anchors <- FindIntegrationAnchors(object.list = pancreas.list)
# integrate embeddings
integrated <- IntegrateEmbeddings(anchorset = anchors, reductions = "pca")
}

}
