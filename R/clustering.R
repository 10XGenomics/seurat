#' @include generics.R
#'
NULL

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# Functions
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# Methods for Seurat-defined generics
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

#' @importFrom pbapply pblapply
#' @importFrom future.apply future_lapply
#' @importFrom future nbrOfWorkers
#'
#' @param modularity.fxn Modularity function (1 = standard; 2 = alternative).
#' @param initial.membership,weights,node.sizes Parameters to pass to the Python leidenalg function.
#' @param resolution Value of the resolution parameter, use a value above
#' (below) 1.0 if you want to obtain a larger (smaller) number of communities.
#' @param algorithm Algorithm for modularity optimization (1 = original Louvain
#' algorithm; 2 = Louvain algorithm with multilevel refinement; 3 = SLM
#' algorithm; 4 = Leiden algorithm). Leiden requires the leidenalg python.
#' @param method Method for running leiden (defaults to matrix which is fast for small datasets).
#' Enable method = "igraph" to avoid casting large data to a dense matrix.
#' @param n.start Number of random starts.
#' @param n.iter Maximal number of iterations per random start.
#' @param random.seed Seed of the random number generator.
#' @param group.singletons Group singletons into nearest cluster. If FALSE, assign all singletons to
#' a "singleton" group
#' @param temp.file.location Directory where intermediate files will be written.
#' Specify the ABSOLUTE path.
#' @param edge.file.name Edge file to use as input for modularity optimizer jar.
#' @param verbose Print output
#'
#' @rdname FindClusters
#' @export
#'
FindClusters.default <- function(
  object,
  modularity.fxn = 1,
  initial.membership = NULL,
  weights = NULL,
  node.sizes = NULL,
  resolution = 0.8,
  method = "matrix",
  algorithm = 1,
  n.start = 10,
  n.iter = 10,
  random.seed = 0,
  group.singletons = TRUE,
  temp.file.location = NULL,
  edge.file.name = NULL,
  verbose = TRUE,
  ...
) {
  CheckDots(...)
  if (is.null(x = object)) {
    stop("Please provide an SNN graph")
  }
  if (tolower(x = algorithm) == "louvain") {
    algorithm <- 1
  }
  if (tolower(x = algorithm) == "leiden") {
    algorithm <- 4
  }
  if (nbrOfWorkers() > 1) {
    clustering.results <- future_lapply(
      X = resolution,
      FUN = function(r) {
        if (algorithm %in% c(1:3)) {
          ids <- RunModularityClustering(
            SNN = object,
            modularity = modularity.fxn,
            resolution = r,
            algorithm = algorithm,
            n.start = n.start,
            n.iter = n.iter,
            random.seed = random.seed,
            print.output = verbose,
            temp.file.location = temp.file.location,
            edge.file.name = edge.file.name
          )
        } else if (algorithm == 4) {
          ids <- RunLeiden(
            object = object,
            method = method,
            partition.type = "RBConfigurationVertexPartition",
            initial.membership = initial.membership,
            weights = weights,
            node.sizes = node.sizes,
            resolution.parameter = r,
            random.seed = random.seed,
            n.iter = n.iter
          )
        } else {
          stop("algorithm not recognised, please specify as an integer or string")
        }
        names(x = ids) <- colnames(x = object)
        ids <- GroupSingletons(ids = ids, SNN = object, verbose = verbose)
        results <- list(factor(x = ids))
        names(x = results) <- paste0('res.', r)
        return(results)
      }
    )
    clustering.results <- as.data.frame(x = clustering.results)
  } else {
    clustering.results <- data.frame(row.names = colnames(x = object))
    for (r in resolution) {
      if (algorithm %in% c(1:3)) {
        ids <- RunModularityClustering(
          SNN = object,
          modularity = modularity.fxn,
          resolution = r,
          algorithm = algorithm,
          n.start = n.start,
          n.iter = n.iter,
          random.seed = random.seed,
          print.output = verbose,
          temp.file.location = temp.file.location,
          edge.file.name = edge.file.name)
      } else if (algorithm == 4) {
        ids <- RunLeiden(
          object = object,
          method = method,
          partition.type = "RBConfigurationVertexPartition",
          initial.membership = initial.membership,
          weights = weights,
          node.sizes = node.sizes,
          resolution.parameter = r,
          random.seed = random.seed,
          n.iter = n.iter
        )
      } else {
        stop("algorithm not recognised, please specify as an integer or string")
      }
      names(x = ids) <- colnames(x = object)
      ids <- GroupSingletons(ids = ids, SNN = object, group.singletons = group.singletons, verbose = verbose)
      clustering.results[, paste0("res.", r)] <- factor(x = ids)
    }
  }
  return(clustering.results)
}

#' @importFrom methods is
#'
#' @param graph.name Name of graph to use for the clustering algorithm
#'
#' @rdname FindClusters
#' @export
#' @method FindClusters Seurat
#'
FindClusters.Seurat <- function(
  object,
  graph.name = NULL,
  modularity.fxn = 1,
  initial.membership = NULL,
  weights = NULL,
  node.sizes = NULL,
  resolution = 0.8,
  method = "matrix",
  algorithm = 1,
  n.start = 10,
  n.iter = 10,
  random.seed = 0,
  group.singletons = TRUE,
  temp.file.location = NULL,
  edge.file.name = NULL,
  verbose = TRUE,
  ...
) {
  CheckDots(...)
  graph.name <- graph.name %||% paste0(DefaultAssay(object = object), "_snn")
  if (!graph.name %in% names(x = object)) {
    stop("Provided graph.name not present in Seurat object")
  }
  if (!is(object = object[[graph.name]], class2 = "Graph")) {
    stop("Provided graph.name does not correspond to a graph object.")
  }
  clustering.results <- FindClusters(
    object = object[[graph.name]],
    modularity.fxn = modularity.fxn,
    initial.membership = initial.membership,
    weights = weights,
    node.sizes = node.sizes,
    resolution = resolution,
    method = method,
    algorithm = algorithm,
    n.start = n.start,
    n.iter = n.iter,
    random.seed = random.seed,
    group.singletons = group.singletons,
    temp.file.location = temp.file.location,
    edge.file.name = edge.file.name,
    verbose = verbose,
    ...
  )
  colnames(x = clustering.results) <- paste0(graph.name, "_", colnames(x = clustering.results))
  object <- AddMetaData(object = object, metadata = clustering.results)
  Idents(object = object) <- colnames(x = clustering.results)[ncol(x = clustering.results)]
  levels <- levels(x = object)
  levels <- tryCatch(
    expr = as.numeric(x = levels),
    warning = function(...) {
      return(levels)
    },
    error = function(...) {
      return(levels)
    }
  )
  Idents(object = object) <- factor(x = Idents(object = object), levels = sort(x = levels))
  object[['seurat_clusters']] <- Idents(object = object)
  cmd <- LogSeuratCommand(object = object, return.command = TRUE)
  slot(object = cmd, name = 'assay.used') <- DefaultAssay(object = object[[graph.name]])
  object[[slot(object = cmd, name = 'name')]] <- cmd
  return(object)
}

#' @param distance.matrix Boolean value of whether the provided matrix is a
#' distance matrix; note, for objects of class \code{dist}, this parameter will
#' be set automatically
#' @param k.param Defines k for the k-nearest neighbor algorithm
#' @param compute.SNN also compute the shared nearest neighbor graph
#' @param prune.SNN Sets the cutoff for acceptable Jaccard index when
#' computing the neighborhood overlap for the SNN construction. Any edges with
#' values less than or equal to this will be set to 0 and removed from the SNN
#' graph. Essentially sets the strigency of pruning (0 --- no pruning, 1 ---
#' prune everything).
#' @param nn.method Method for nearest neighbor finding. Options include: rann,
#' annoy
#' @param annoy.metric Distance metric for annoy. Options include: euclidean,
#' cosine, manhattan, and hamming
#' @param nn.eps Error bound when performing nearest neighbor seach using RANN;
#' default of 0.0 implies exact nearest neighbor search
#' @param verbose Whether or not to print output to the console
#' @param force.recalc Force recalculation of SNN.
#'
#' @importFrom RANN nn2
#' @importFrom methods as
#'
#' @rdname FindNeighbors
#' @export
#' @method FindNeighbors default
#'
FindNeighbors.default <- function(
  object,
  distance.matrix = FALSE,
  k.param = 20,
  compute.SNN = TRUE,
  prune.SNN = 1/15,
  nn.method = 'rann',
  annoy.metric = "euclidean",
  nn.eps = 0,
  verbose = TRUE,
  force.recalc = FALSE,
  ...
) {
  CheckDots(...)
  if (is.null(x = dim(x = object))) {
    warning(
      "Object should have two dimensions, attempting to coerce to matrix",
      call. = FALSE
    )
    object <- as.matrix(x = object)
  }
  if (is.null(rownames(x = object))) {
    stop("Please provide rownames (cell names) with the input object")
  }
  n.cells <- nrow(x = object)
  if (n.cells < k.param) {
    warning(
      "k.param set larger than number of cells. Setting k.param to number of cells - 1.",
      call. = FALSE
    )
    k.param <- n.cells - 1
  }
  # find the k-nearest neighbors for each single cell
  if (!distance.matrix) {
    if (verbose) {
      message("Computing nearest neighbor graph")
    }
    nn.ranked <- NNHelper(
      data = object,
      k = k.param,
      method = nn.method,
      searchtype = "standard",
      eps = nn.eps,
      metric = annoy.metric)
    nn.ranked <- nn.ranked$nn.idx
  } else {
    if (verbose) {
      message("Building SNN based on a provided distance matrix")
    }
    knn.mat <- matrix(data = 0, ncol = k.param, nrow = n.cells)
    knd.mat <- knn.mat
    for (i in 1:n.cells) {
      knn.mat[i, ] <- order(object[i, ])[1:k.param]
      knd.mat[i, ] <- object[i, knn.mat[i, ]]
    }
    nn.ranked <- knn.mat[, 1:k.param]
  }
  # convert nn.ranked into a Graph
  j <- as.numeric(x = t(x = nn.ranked))
  i <- ((1:length(x = j)) - 1) %/% k.param + 1
  nn.matrix <- as(object = sparseMatrix(i = i, j = j, x = 1, dims = c(nrow(x = object), nrow(x = object))), Class = "Graph")
  rownames(x = nn.matrix) <- rownames(x = object)
  colnames(x = nn.matrix) <- rownames(x = object)
  neighbor.graphs <- list(nn = nn.matrix)
  if (compute.SNN) {
    if (verbose) {
      message("Computing SNN")
    }
    snn.matrix <- ComputeSNN(
      nn_ranked = nn.ranked,
      prune = prune.SNN
    )
    rownames(x = snn.matrix) <- rownames(x = object)
    colnames(x = snn.matrix) <- rownames(x = object)
    snn.matrix <- as.Graph(x = snn.matrix)
    neighbor.graphs[["snn"]] <- snn.matrix
  }
  return(neighbor.graphs)
}

#' @rdname FindNeighbors
#' @export
#' @method FindNeighbors Assay
#'
FindNeighbors.Assay <- function(
  object,
  features = NULL,
  k.param = 20,
  compute.SNN = TRUE,
  prune.SNN = 1/15,
  nn.method = 'rann',
  annoy.metric = "euclidean",
  nn.eps = 0,
  verbose = TRUE,
  force.recalc = FALSE,
  ...
) {
  CheckDots(...)
  features <- features %||% VariableFeatures(object = object)
  data.use <- t(x = GetAssayData(object = object, slot = "data")[features, ])
  neighbor.graphs <- FindNeighbors(
    object = data.use,
    k.param = k.param,
    compute.SNN = compute.SNN,
    prune.SNN = prune.SNN,
    nn.method = nn.method,
    annoy.metric = annoy.metric,
    nn.eps = nn.eps,
    verbose = verbose,
    force.recalc = force.recalc,
    ...
  )
  return(neighbor.graphs)
}

#' @rdname FindNeighbors
#' @export
#' @method FindNeighbors dist
#'
FindNeighbors.dist <- function(
  object,
  k.param = 20,
  compute.SNN = TRUE,
  prune.SNN = 1/15,
  nn.method = "rann",
  annoy.metric = "euclidean",
  nn.eps = 0,
  verbose = TRUE,
  force.recalc = FALSE,
  ...
) {
  CheckDots(...)
  return(FindNeighbors(
    object = as.matrix(x = object),
    distance.matrix = TRUE,
    k.param = k.param,
    compute.SNN = compute.SNN,
    prune.SNN = prune.SNN,
    nn.eps = nn.eps,
    nn.method = nn.method,
    annoy.metric = annoy.metric,
    verbose = verbose,
    force.recalc = force.recalc,
    ...
  ))
}

#' @param assay Assay to use in construction of SNN
#' @param features Features to use as input for building the SNN
#' @param reduction Reduction to use as input for building the SNN
#' @param dims Dimensions of reduction to use as input
#' @param do.plot Plot SNN graph on tSNE coordinates
#' @param graph.name Optional naming parameter for stored SNN graph. Default is
#' assay.name_snn.
#'
#' @importFrom igraph graph.adjacency plot.igraph E
#'
#' @rdname FindNeighbors
#' @export
#' @method FindNeighbors Seurat
#'
FindNeighbors.Seurat <- function(
  object,
  reduction = "pca",
  dims = 1:10,
  assay = NULL,
  features = NULL,
  k.param = 20,
  compute.SNN = TRUE,
  prune.SNN = 1/15,
  nn.method = "rann",
  annoy.metric = "euclidean",
  nn.eps = 0,
  verbose = TRUE,
  force.recalc = FALSE,
  do.plot = FALSE,
  graph.name = NULL,
  ...
) {
  CheckDots(...)
  if (!is.null(x = dims)) {
    # assay <- assay %||% DefaultAssay(object = object)
    assay <- DefaultAssay(object = object[[reduction]])
    data.use <- Embeddings(object = object[[reduction]])
    if (max(dims) > ncol(x = data.use)) {
      stop("More dimensions specified in dims than have been computed")
    }
    data.use <- data.use[, dims]
    neighbor.graphs <- FindNeighbors(
      object = data.use,
      k.param = k.param,
      compute.SNN = compute.SNN,
      prune.SNN = prune.SNN,
      nn.method = nn.method,
      annoy.metric = annoy.metric,
      nn.eps = nn.eps,
      verbose = verbose,
      force.recalc = force.recalc,
      ...
    )
  } else {
    assay <- assay %||% DefaultAssay(object = object)
    data.use <- GetAssay(object = object, assay = assay)
    neighbor.graphs <- FindNeighbors(
      object = data.use,
      features = features,
      k.param = k.param,
      compute.SNN = compute.SNN,
      prune.SNN = prune.SNN,
      nn.method = nn.method,
      annoy.metric = annoy.metric,
      nn.eps = nn.eps,
      verbose = verbose,
      force.recalc = force.recalc,
      ...
    )
  }
  graph.name <- graph.name %||% paste0(assay, "_", names(x = neighbor.graphs))
  for (ii in 1:length(x = graph.name)) {
    DefaultAssay(object = neighbor.graphs[[ii]]) <- assay
    object[[graph.name[[ii]]]] <- neighbor.graphs[[ii]]
  }
  if (do.plot) {
    if (!"tsne" %in% names(x = object@reductions)) {
      warning("Please compute a tSNE for SNN visualization. See RunTSNE().")
    } else {
      if (nrow(x = Embeddings(object = object[["tsne"]])) != ncol(x = object)) {
        warning("Please compute a tSNE for SNN visualization. See RunTSNE().")
      } else {
        net <- graph.adjacency(
          adjmatrix = as.matrix(x = neighbor.graphs[[2]]),
          mode = "undirected",
          weighted = TRUE,
          diag = FALSE
        )
        plot.igraph(
          x = net,
          layout = as.matrix(x = Embeddings(object = object[["tsne"]])),
          edge.width = E(graph = net)$weight,
          vertex.label = NA,
          vertex.size = 0
        )
      }
    }
  }
  object <- LogSeuratCommand(object = object)
  return(object)
}

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# Internal
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# Run annoy
#
# @param data Data to build the index with
# @param query A set of data to be queried against data
# @param metric Distance metric; can be one of "euclidean", "cosine", "manhattan",
# "hamming"
# @param n.trees More trees gives higher precision when querying
# @param k Number of neighbors
# @param search.k During the query it will inspect up to search_k nodes which
# gives you a run-time tradeoff between better accuracy and speed.
# @ param include.distance Include the corresponding distances
#
AnnoyNN <- function(data, query = data, metric = "euclidean", n.trees = 50, k,
                    search.k = -1, include.distance = TRUE) {
  idx <- AnnoyBuildIndex(
    data = data,
    metric = metric,
    n.trees = n.trees)
  nn <- AnnoySearch(
    index = idx,
    query = query,
    k = k,
    search.k = search.k,
    include.distance = include.distance)
  return(nn)
}

# Build the annoy index
#
# @param data Data to build the index with
# @param metric Distance metric; can be one of "euclidean", "cosine", "manhattan",
# "hamming"
# @param n.trees More trees gives higher precision when querying
#' @importFrom RcppAnnoy AnnoyEuclidean AnnoyAngular AnnoyManhattan AnnoyHamming
#
AnnoyBuildIndex <- function(data, metric = "euclidean", n.trees = 50) {
  f <- ncol(x = data)
  a <- switch(
    EXPR = metric,
    "euclidean" =  new(Class = RcppAnnoy::AnnoyEuclidean, f),
    "cosine" = new(Class = RcppAnnoy::AnnoyAngular, f),
    "manhattan" = new(Class = RcppAnnoy::AnnoyManhattan, f),
    "hamming" = new(Class = RcppAnnoy::AnnoyHamming, f),
    stop ("Invalid metric")
  )
  for (ii in seq(nrow(x = data))) {
    a$addItem(ii - 1, data[ii, ])
  }
  a$build(n.trees)
  return(a)
}

# Search the annoy index
#
# @param Annoy index, build with AnnoyBuildIndex
# @param query A set of data to be queried against the index
# @param k Number of neighbors
# @param search.k During the query it will inspect up to search_k nodes which
# gives you a run-time tradeoff between better accuracy and speed.
# @ param include.distance Include the corresponding distances
#
AnnoySearch <- function(index, query, k, search.k = -1, include.distance = TRUE) {
  n <- nrow(x = query)
  idx <- matrix(nrow = n,  ncol = k)
  dist <- matrix(nrow = n, ncol = k)
  convert <- methods::is(index, "Rcpp_AnnoyAngular")
  res <- future_lapply(X = 1:n, FUN = function(x) {
    res <- index$getNNsByVectorList(query[x, ], k, search.k, include.distance)
    # Convert from Angular to Cosine distance
    if (convert) {
      res$dist <- 0.5 * (res$dist * res$dist)
    }
    list(res$item + 1, res$distance)
  })
  for (i in 1:n) {
    idx[i, ] <- res[[i]][[1]]
    if (include.distance) {
      dist[i, ] <- res[[i]][[2]]
    }
  }
  return(list(nn.idx = idx, nn.dists = dist))
}

# Group single cells that make up their own cluster in with the cluster they are
# most connected to.
#
# @param ids Named vector of cluster ids
# @param SNN SNN graph used in clustering
# @param group.singletons Group singletons into nearest cluster. If FALSE, assign all singletons to
# a "singleton" group
#
# @return Returns Seurat object with all singletons merged with most connected cluster
#
GroupSingletons <- function(ids, SNN, group.singletons = TRUE, verbose = TRUE) {
  # identify singletons
  singletons <- c()
  singletons <- names(x = which(x = table(ids) == 1))
  singletons <- intersect(x = unique(x = ids), singletons)
  if (!group.singletons) {
    ids[which(ids %in% singletons)] <- "singleton"
    return(ids)
  }
  # calculate connectivity of singletons to other clusters, add singleton
  # to cluster it is most connected to
  cluster_names <- as.character(x = unique(x = ids))
  cluster_names <- setdiff(x = cluster_names, y = singletons)
  connectivity <- vector(mode = "numeric", length = length(x = cluster_names))
  names(x = connectivity) <- cluster_names
  new.ids <- ids
  for (i in singletons) {
    i.cells <- names(which(ids == i))
    for (j in cluster_names) {
      j.cells <- names(which(ids == j))
      subSNN <- SNN[i.cells, j.cells]
      set.seed(1) # to match previous behavior, random seed being set in WhichCells
      if (is.object(x = subSNN)) {
        connectivity[j] <- sum(subSNN) / (nrow(x = subSNN) * ncol(x = subSNN))
      } else {
        connectivity[j] <- mean(x = subSNN)
      }
    }
    m <- max(connectivity, na.rm = T)
    mi <- which(x = connectivity == m, arr.ind = TRUE)
    closest_cluster <- sample(x = names(x = connectivity[mi]), 1)
    ids[i.cells] <- closest_cluster
  }
  if (length(x = singletons) > 0 && verbose) {
    message(paste(
      length(x = singletons),
      "singletons identified.",
      length(x = unique(x = ids)),
      "final clusters."
    ))
  }
  return(ids)
}

# Internal helper function to dispatch to various neighbor finding methods
#
# @param data Input data
# @param query Data to query against data
# @param k Number of nearest neighbors to compute
# @param method Nearest neighbor method to use: "rann", "annoy"
# @param ... additional parameters to specific neighbor finding method
#
NNHelper <- function(data, query = data, k, method, ...) {
  args <- as.list(x = sys.frame(which = sys.nframe()))
  args <- c(args, list(...))
  return(
    switch(
      EXPR = method,
      "rann" = {
        args <- args[intersect(x = names(x = args), y = names(x = formals(fun = nn2)))]
        do.call(what = 'nn2', args = args)
      },
      "annoy" = {
        args <- args[intersect(x = names(x = args), y = names(x = formals(fun = AnnoyNN)))]
        do.call(what = 'AnnoyNN', args = args)
      },
      stop("Invalid method. Please choose one of 'rann', 'annoy'")
    )
  )
}

# Run Leiden clustering algorithm
#
# Implements the Leiden clustering algorithm in R using reticulate
# to run the Python version. Requires the python "leidenalg" and "igraph" modules
# to be installed. Returns a vector of partition indices.
#
# @param adj_mat An adjacency matrix or SNN matrix
# @param partition.type Type of partition to use for Leiden algorithm.
# Defaults to RBConfigurationVertexPartition. Options include: ModularityVertexPartition,
# RBERVertexPartition, CPMVertexPartition, MutableVertexPartition,
# SignificanceVertexPartition, SurpriseVertexPartition (see the Leiden python
# module documentation for more details)
# @param initial.membership,weights,node.sizes Parameters to pass to the Python leidenalg function.
# @param resolution.parameter A parameter controlling the coarseness of the clusters
# for Leiden algorithm. Higher values lead to more clusters. (defaults to 1.0 for
# partition types that accept a resolution parameter)
# @param random.seed Seed of the random number generator
# @param n.iter Maximal number of iterations per random start
#
# @keywords graph network igraph mvtnorm simulation
#
#' @importFrom leiden leiden
#' @importFrom reticulate py_module_available
#' @importFrom igraph graph_from_adjacency_matrix graph_from_adj_list
#
# @author Tom Kelly
#
# @export
#
RunLeiden <- function(
  object,
  method = c("matrix", "igraph"),
  partition.type = c(
    'RBConfigurationVertexPartition',
    'ModularityVertexPartition',
    'RBERVertexPartition',
    'CPMVertexPartition',
    'MutableVertexPartition',
    'SignificanceVertexPartition',
    'SurpriseVertexPartition'
  ),
  initial.membership = NULL,
  weights = NULL,
  node.sizes = NULL,
  resolution.parameter = 1,
  random.seed = 0,
  n.iter = 10
) {
  if (!py_module_available(module = 'leidenalg')) {
    stop(
      "Cannot find Leiden algorithm, please install through pip (e.g. pip install leidenalg).",
      call. = FALSE
    )
  }
  switch(
    EXPR = method,
    "matrix" = {
      input <- as(object = object, Class = "matrix")
      },
    "igraph" = {
      input <- if (inherits(x = object, what = 'list')) {
        if (is.null(x = weights)) {
          graph_from_adj_list(adjlist = object)
        } else {
          graph_from_adj_list(adjlist = object)
        }
      } else if (inherits(x = object, what = c('dgCMatrix', 'matrix', "Matrix"))) {
        if (is.null(x = weights)) {
          graph_from_adjacency_matrix(adjmatrix = object)
        } else {
          graph_from_adjacency_matrix(adjmatrix = object, weighted = TRUE)
        }
      } else if (inherits(x = object, what = 'igraph')) {
        object
      } else {
        stop(
          "Method for Leiden not found for class", class(x = object), 
           call. = FALSE
        )
      }
    },
    stop("Method for Leiden must be either 'matrix' or igraph'")
  )
  #run leiden from CRAN package (calls python with reticulate)
  partition <- leiden(
    object = input,
    partition_type = partition.type,
    initial_membership = initial.membership,
    weights = weights,
    node_sizes = node.sizes,
    resolution_parameter = resolution.parameter,
    seed = random.seed,
    n_iterations = n.iter
  )
  return(partition)
}

# Runs the modularity optimizer (C++ port of java program ModularityOptimizer.jar)
#
# @param SNN SNN matrix to use as input for the clustering algorithms
# @param modularity Modularity function to use in clustering (1 = standard; 2 = alternative)
# @param resolution Value of the resolution parameter, use a value above (below) 1.0 if you want to obtain a larger (smaller) number of communities
# @param algorithm Algorithm for modularity optimization (1 = original Louvain algorithm; 2 = Louvain algorithm with multilevel refinement; 3 = SLM algorithm; 4 = Leiden algorithm). Leiden requires the leidenalg python module.
# @param n.start Number of random starts
# @param n.iter Maximal number of iterations per random start
# @param random.seed Seed of the random number generator
# @param print.output Whether or not to print output to the console
# @param temp.file.location Deprecated and no longer used
# @param edge.file.name Path to edge file to use
#
# @return Seurat object with identities set to the results of the clustering procedure
#
#' @importFrom utils read.table write.table
#
RunModularityClustering <- function(
  SNN = matrix(),
  modularity = 1,
  resolution = 0.8,
  algorithm = 1,
  n.start = 10,
  n.iter = 10,
  random.seed = 0,
  print.output = TRUE,
  temp.file.location = NULL,
  edge.file.name = NULL
) {
  edge_file <- edge.file.name %||% ''
  clusters <- RunModularityClusteringCpp(
    SNN,
    modularity,
    resolution,
    algorithm,
    n.start,
    n.iter,
    random.seed,
    print.output,
    edge_file
  )
  return(clusters)
}



# Find subclusters under one cluster
#' @param object a seurat object
#' @param cluster the cluster needed to be subclustered
#' @param graph.name the name of graph used for clustering
#' @param subcluster.name the name of sub cluster added in the meta.data
#' @param resolution.sub Value of the resolution parameter, use a value above
#' (below) 1.0 if you want to obtain a larger (smaller) number of communities.
#' @param algorithm Algorithm for modularity optimization (1 = original Louvain
#' algorithm; 2 = Louvain algorithm with multilevel refinement; 3 = SLM
#' algorithm; 4 = Leiden algorithm). Leiden requires the leidenalg python.

FindSubCluster <- function(object, 
                           cluster, 
                           graph.name,
                           subcluster.name = "sub.cluster",
                           resolution.sub = 1, 
                           algorithm = 1
){
  sub.obj <- subset(object, idents = cluster)
  sub.obj[[graph.name]]   <- as.Graph(object[[graph.name]][Cells(sub.obj), Cells(sub.obj)])
  sub.obj <- FindClusters(sub.obj, graph.name = graph.name, resolution = resolution.sub, algorithm = algorithm)
  Idents(sub.obj) <- paste(cluster , Idents(sub.obj), sep = "_")
  
  object@meta.data[, subcluster.name] <- as.character(Idents(object))
  object@meta.data[Cells(sub.obj), subcluster.name] <- as.character(Idents(sub.obj))
  return(object)
}

#' Predict expression value from knn
#'
#' @param object The object used to calculate knn
#' @param nn.idx k near neighbour indices. A cells x k matrix.
#' @param assay Assay used for prediction
#' @param reduction Cell embedding of the reduction used for prediction
#' @param dims Number of dimensions of cell embedding
#' @param slot slot used for prediction
#' @param features features used for prediction
#' @param mean.type the type of mean, 	arithmetic mean (amean) or geometric mean (gmean)
#'
#' @importFrom pbapply pbapply
#' @importFrom future.apply future_apply
#' @importFrom future nbrOfWorkers
#' @importFrom sctransform row_gmean
#'
#' @return return an assay containing predicted expression value in the data slot
#' @export
#' 
PredictAssay <- function(
  object,
  nn.idx,
  assay,
  reduction = NULL,
  dims = NULL, 
  return.assay = TRUE,
  cells = NULL,
  slot = "scale.data",
  features = NULL,
  mean.type = NULL,
  verbose = TRUE
){
  if(is.null(mean.type)){
    mean.type <- "amean"
  }
  if(is.null(reduction)){
    reference.data <- GetAssayData(object = object,
                                   assay = assay,
                                   slot = slot)
    if(is.null(features)){
      features <- VariableFeatures(object[[assay]])
      if (length(features) == 0){
        message("VariableFeatures are empty in the ",assay," assay, features in the ", slot, " slot will be used" )
        features <- rownames(reference.data)
        if (length(features) == 0){
          stop("No features in the ",slot, " slot of the assay ",assay )
        }
      }
    }
    reference.data <- reference.data[features, ]
  } else{
    if(is.null(dims)){
      stop("dims is empty")
    }
    reference.data <- t(object[[reduction]]@cell.embeddings[,dims ])
  }
  if(nrow(nn.idx) > 1){
    if ( all(nn.idx[1:10,1] == 1:10 )){
      if(verbose){
        message("The nearest neighbor is the query cell itself, and it will not be used for prediction")
      }
      nn.idx <- nn.idx[,-1]
    }
    if (mean.type =="gmean") {
      predicted <- apply(nn.idx,
                         1,
                         function(x) sctransform:::row_gmean(reference.data[,x]))
    }else if (mean.type == "ExpMean") {
      predicted <- apply(nn.idx,
                         1,
                         function(x) FastExpMean(reference.data[,x], display_progress = F))
    } else {
      predicted <- apply(nn.idx,
                         1,
                         function(x) rowMeans(reference.data[,x], 
                                              na.rm = T)    )
    } 
  } else if(nrow(nn.idx) ==1){
    if(verbose){
      message("Only one cell's neighbors are given, and the first one will not be used for prediction")
    }
    nn.idx <- nn.idx[,-1]
    predicted <-  rowMeans(reference.data[,nn.idx],  na.rm = T)
    predicted <- as.matrix(predicted)
  }
  if(is.null(cells)){
    cells <-  Cells(object)
  }
  colnames(predicted) <- cells
  if(return.assay){
    predicted.assay <- CreateAssayObject(data = predicted)
    return(predicted.assay)
  } else{
    return(predicted)
  }
}



projangular_nn_dist <- function( nn.idx, 
                                 redunction.embedding, 
                                 query.reduction.embedding = NULL){
  
  if( !is.list(nn.idx) ){
    nn.idx <- lapply(1:nrow(nn.idx), function(x) nn.idx[x,])
  }
  if(is.null( query.reduction.embedding)){
    query.reduction.embedding <- redunction.embedding
  }
  nn.dist <- lapply(X = 1:nrow(query.reduction.embedding), 
                       FUN = function(x){
                         correlation = as.matrix(  rdist::cdist( X = query.reduction.embedding[x,,drop = F], 
                                                                 Y = redunction.embedding[  nn.idx[[x]],], 
                                                                 metric = "euclidean") )
                       })
  return( nn.dist )
}


# Find multi-model neighbors 
#
#' @param object The object used to calculate knn
#' @param k.nn .Number of nearest multi-model neighbors to compute
#' @param reduction.list A list of reduction name 
#' @param dims.list A list of dimentions used for the reduction
#' @param knn.range The number of approximate neighbors to compute
#' @param modality.weight the cell-specific modeality weights
#' @param verbose Whether or not to print output to the console
#'
#' @return return a list containing nn index and nn multi-model distance
#' @export
#' 
MultiModelNN <- function(object, 
                         query = NULL,
                         k.nn = 20, 
                         reduction.list,
                         dims.list = NULL,
                         knn.range = 200,
                         modality.weight = NULL,
                         sigma.list = NULL,
                         l2.norm = FALSE, 
                         verbose = TRUE
){
  if( is.null(modality.weight)){
    modality.weight <- lapply(X = 1:length(reduction.list),
                              FUN = function(r) rep( 1/length(reduction.list), ncol(object)) )
  }
  if(!is.list(modality.weight)){
    modality.weight <- list(modality.weight, (1 - modality.weight))
  }
  if( class(object)[1] == "Seurat"){
    redunction_embedding <- lapply( X = 1:length(reduction.list), 
                                    FUN = function(x) {
                                      Embeddings(object[[reduction.list[[x]] ]])[ ,dims.list[[x]] ]
                                    })
  } else {
    redunction_embedding <- object
  }
  
  if(is.null(query)){
    query.redunction_embedding <- redunction_embedding
    query <- object
  } else{
    if( class(query)[1] == "Seurat" ){
      query.redunction_embedding <- lapply( X = 1:length(reduction.list), 
                                            FUN = function(x) {
                                              Embeddings(query[[reduction.list[[x]] ]])[ ,dims.list[[x]] ]
                                            })
    } else {
      query.redunction_embedding <- query
    }
  }
  if(l2.norm){
    query.redunction_embedding <- lapply( query.redunction_embedding, function(x)  L2Norm(x))
    redunction_embedding <-lapply( redunction_embedding, function(x)  L2Norm(x))
  }
  query.cell.num <- nrow( query.redunction_embedding[[1]] )
  reduction.num <- length( query.redunction_embedding )
  redunction_nn <- lapply( X = 1:reduction.num, 
                           FUN = function(x){
                             NNHelper(data = redunction_embedding[[x]], 
                                      query = query.redunction_embedding[[x]],
                                      k = knn.range,
                                      method = 'annoy',
                                      metric = "euclidean") })
  # union of rna and adt nn, remove itself from neighobors
  redunction_nn <- lapply(X = redunction_nn , 
                          FUN = function(x)  x$nn.idx[, -1]  )
  
  nn_idx <- lapply( X = 1:query.cell.num , 
                    FUN = function(x)  Reduce(union,lapply(redunction_nn, function(y) y[x,] )))
  # calculate cosine similarity of union neighobors, and convert to angular and projection similarity
  nn_dist <- lapply(X = 1:reduction.num,  
                                       FUN = function(r){
                                         projangular_nn_dist(nn.idx = nn_idx,
                                                             redunction.embedding = redunction_embedding[[r]], 
                                                             query.reduction.embedding = query.redunction_embedding[[r]])
                                       })
  # modality weighted distance
    if(length(sigma.list[[1]] ) ==1){
      sigma.list <- lapply(sigma.list, function(x) rep(x = x, ncol(object) ))
    }
    nn_weighted_dist <- lapply(X = 1:reduction.num,  
                               FUN = function(r){
                                 lapply(  X = 1:query.cell.num,
                                          FUN = function(x){ 
                                             exp(-1*(nn_dist[[r]][[x]] / sigma.list[[r]][x] )**
                                                       2) * 
                                              modality.weight[[r]][x] })
                               })
    nn_weighted_dist <- sapply(X = 1:query.cell.num, 
                               FUN =  function(x){ 
                                 Reduce("+", 
                                        lapply( X = 1:reduction.num, 
                                                FUN = function(r) nn_weighted_dist[[r]][[x]] )) 
                               })
    
     

  # select k nearest joint neighbors
  select_idx <-  lapply( X = nn_weighted_dist,
                         FUN = function(dist){
                           which(rank(dist*-1,  ties.method = "first") <= (k.nn))
                         })
  select_nn <- t(sapply( X = 1:query.cell.num,
                         FUN = function(x) nn_idx[[x]][select_idx[[x]]])
  )
  
  select_dist <- t(sapply( X = 1:query.cell.num, 
                           FUN = function(x) nn_weighted_dist[[x]][select_idx[[x]]])
  )
  rownames(select_nn) <- rownames(select_dist) <- Cells(query)
  joint.nn <- list(select_nn, select_dist)
  names(joint.nn) <- c("nn.idx", "nn.dists")
  return(joint.nn)
}


#' Multimodel KNN and SNN Graph Construction
#' 
#' @param object The object used to calculate knn
#' @param reduction.list A list of reduction name 
#' @param dims.list A list of dimentions used for the reduction
#' @param k.nn .Number of nearest multi-model neighbors to compute
#' @param knn.range The number of approximate neighbors to compute
#' @param knn.graph.name The name of multi-model knn graph
#' @param snn.graph.name The name of multi-model snn graph
#' @param joint.nn.name The name of multi-model neighbors
#' @param modality.weight the cell-specific modeality weights
#' @param verbose Whether or not to print output to the console
#' 
#' @return return an object containing multi-model KNN, SNN and neighbors
#' @export
FindMultiModelNeighbors <-  function(object, 
                                     reduction.list = NULL,
                                     dims.list = NULL,
                                     k.nn = 20, 
                                     knn.range = 200,
                                     weighted.graph = FALSE,
                                     l2.norm = FALSE, 
                                     knn.graph.name = "jknn",
                                     snn.graph.name = "jsnn",
                                     joint.nn.name = "joint.nn",
                                     prune.SNN = 1/20, 
                                     modality.weight = NULL,
                                     verbose = TRUE
){
  if( is.list(modality.weight) ){
    l2.norm <- modality.weight$params$l2.norm
    sigma.list <- modality.weight$params$sigma.list
    if(is.null(reduction.list)){
      reduction.list <- modality.weight$params$reduction.list
    }
    if(is.null(dims.list)){
      dims.list <- modality.weight$params$dims.list
    }
    modality.weight <- modality.weight$first.modality.weight
    }
  
  joint.nn <- MultiModelNN(object = object, 
                      k.nn = k.nn, 
                      reduction.list = reduction.list, 
                      dims.list = dims.list,
                      knn.range = knn.range, 
                      modality.weight = modality.weight, 
                      l2.norm = l2.norm, 
                      sigma.list = sigma.list, 
                      verbose = verbose )

  select_nn <- joint.nn$nn.idx
  select_nn_dist <- joint.nn$nn.dists 
if(weighted.graph){
  joint.nn$nn.dists <- t(apply(X = joint.nn$nn.dists, MARGIN = 1, function(x)  x/max(x) ))
    nn.matrix <- sparseMatrix(i = 1:ncol(object), j =1:ncol(object), x = 1)
for (i in 1:ncol(object)){
  nn.matrix[i, select_nn[i,]] <- joint.nn$nn.dists[i, ]
}

  }else{
    j <- as.numeric(x = t(x = select_nn ))
    i <- ((1:length(x = j)) - 1) %/% k.nn + 1
    nn.matrix <- sparseMatrix(i = i,
                                          j = j,
                                          x = 1, 
                                          dims = c(ncol(x = object), ncol(x = object)))
    diag(nn.matrix) <- 1
    }
  rownames(x = nn.matrix) <-  colnames(x = nn.matrix) <- colnames(x = object)
  nn.matrix <- nn.matrix + t(nn.matrix) - t(nn.matrix)*nn.matrix
  nn.matrix <- as.Graph(nn.matrix)
  suppressWarnings(object[[knn.graph.name]] <- nn.matrix)
  snn.matrix <- ComputeSNN(nn_ranked = select_nn, prune = prune.SNN )
  rownames(snn.matrix) <- colnames(snn.matrix) <- Cells(object)
  suppressWarnings(object[[snn.graph.name]] <- as(object = snn.matrix, Class = "Graph"))
  suppressWarnings(  Misc(object, slot = joint.nn.name) <- joint.nn)
  return(object)
}



RunMultiModelReduction <- function(object, 
                                   modality.weight, 
                                   reduction.name, 
                                   reduction.key
){
  if(length(modality.weight) == 3){
    sigma.list <- modality.weight$params$sigma
    sigma.list.flip <- sigma.list
    sigma.list.flip[[1]] <- sigma.list[[2]]
    sigma.list.flip[[2]] <- sigma.list[[1]]
    reduction.list <- modality.weight$params$reduction.list
    dims.list <- modality.weight$params$dims.list
    modality.weight <- modality.weight$first.modality.weight
  }
  if(!is.list(modality.weight)){
    modality.weight <- list( sqrt(modality.weight), sqrt((1 - modality.weight)))
  }
  
  redunction_embedding <- lapply( X = 1:length(reduction.list), 
                                  FUN = function(r) {
                                    Embeddings(object[[reduction.list[[r]] ]])[ ,dims.list[[r]] ]
                                  })
  feature_loading <- lapply( X = 1:length(reduction.list), 
                             FUN = function(r) {
                               Loadings(object[[reduction.list[[r]] ]])[ ,dims.list[[r]] ]
                             })
  
  weighted.redunction_embedding <- lapply( X = 1:length(reduction.list), 
                                  FUN = function(r) {
                                    t(sapply( X = 1:nrow(redunction_embedding[[r]]), 
                                            FUN = function(x) {
                                      redunction_embedding[[r]][x, ] * modality.weight[[r]][x]* sigma.list.flip[[r]]
                                              }))
                                  })
weighted.redunction_embedding <- Reduce(cbind, weighted.redunction_embedding)
rownames(weighted.redunction_embedding) <- Cells(object)
colnames(weighted.redunction_embedding) <- paste0(reduction.key, 1:ncol(weighted.redunction_embedding))
object[[reduction.name]] <- CreateDimReducObject(embeddings = weighted.redunction_embedding, 
                                                 key = reduction.key, 
                                                 assay = DefaultAssay(object))
return(object)
}


#' Calculate modality weights
#'
#' @param object A Seurat object
#' @param assay.list A list of names of assays
#' @param reduction.list A list of name of dimension reduction 
#' @param dims.list A list of number of dimensions to use
#' @param knn.list A list of number of nearest neighbors to use
#' @param type.list A list of name of datatype, such as RNA, ATAC.
#' @param feature.list A list of features for different modality
#' @param slot.list  A list of slots 
#' @param num.boot The number of bootstrapping
#' @param nn.metric distance metric for finding neighbors
#' @param score.c the minimal cross modality prediction similarity
#' @param max.modality_score the maximal modality score
#' @param seed seed for bootstrapping
#' @param verbose Display messages
#'
#' @return Returns a list of similarities
#' @export
#' 
FindModalityWeights <- function(object,
                                assay.list, 
                                reduction.list, 
                                dims.list, 
                                angular.nn = FALSE, 
                                distance = c("proj_angular",  "angular", "projection"  ,"euclidean")[1],
                                kernel = FALSE,
                                kernel.power = 2, 
                                knn.list = NULL,
                                type.list = NULL,
                                num.boot = 2, 
                                nn.metric = "cosine",
                                feature.list = NULL,
                                slot.list = NULL,
                                mean.type = NULL,
                                seed = 1628, 
                                stdev.weight = TRUE, 
                                max.modality_score = 700, 
                                score.c = list(0.1, 0.1),
                                verbose = TRUE
){
  if(!is.list(reduction.list)){
    reduction.list <- as.list(reduction.list)
  }
  if(!is.list(assay.list)){
    assay.list <- as.list(assay.list)
  }
  if( length( unique(unlist(reduction.list))) < length(reduction.list) |
      length( unique(unlist(assay.list))) < length(assay.list) ){
    stop("assay.list and reduction.list require unique input assays and reductions")
  }
  if(!is.list(dims.list)){
    stop("dims.list require a list of dims for reductions")
  }
  if(is.null(knn.list)){
    knn.list <- list(20,20)
  }
  if(is.null(slot.list)){
    slot.list <- list("scale.data", "scale.data")
  }
  if(is.null(type.list)){
    type.list <- list("RNA", "RNA")
  }
  if(is.null(feature.list )){
    feature.list <- lapply( X = 1:length(assay.list), FUN = function(a) rownames( object[[ reduction.list[[a]] ]]@feature.loadings ) )  
  }
  if(!is.list(knn.list)){
    knn.list <- as.list(knn.list)
  }
  if(!is.list(slot.list)){
    slot.list <- as.list(slot.list)
  }
  if(!is.list(type.list)){
    type.list <- as.list(type.list)
  }
  names(score.c) <- names(type.list) <- names(feature.list) <- names(slot.list) <- names(reduction.list) <- names(assay.list) <- names(dims.list) <- names(knn.list) <- unlist(assay.list)
  set.seed(seed)
  proj.embedding.boot <- list()
  proj.loading.boot <- list()
  if(num.boot > 0){
    boot.list <- lapply( X = 1: num.boot,
                         FUN = function(x){
                           sample(x = 1:ncol(object), size = ncol(object), replace = T)
                         })
    total.loss <- list()
    if(verbose){
      message("Building bootstraping PCA")
      pb <- txtProgressBar(min = 0, max = length(boot.list), style = 3)
    }
    for (f in 1:length(boot.list)){
      feature.multi <- feature.list 
      boot.multi <- lapply( X = assay.list, 
                            FUN = function(assay){
                              GetAssayData(object = object, 
                                           assay = assay, 
                                           slot = slot.list[[assay]] )[ feature.multi[[assay]], boot.list[[f]] ]
                            }) 
      boot.pca.multi <- lapply( X = assay.list, 
                                FUN = function(assay){
                                  RunSVD.bootstrapping(object = object,
                                                       boot.object = boot.multi[[assay]], 
                                                       npcs = max(50, dims.list[[assay]]),
                                                       reduction = reduction.list[[assay]],
                                                       type = type.list[[assay]],
                                                       dims =  dims.list[[assay]],
                                                       assay = assay,
                                                       slot = slot.list[[assay]],
                                                       verbose = FALSE)
                                })
      proj.embedding.boot[[f]] <- lapply(X =  assay.list, 
                                         FUN = function(assay){
                                           boot.pca.multi[[assay]]@cell.embeddings
                                         })
      
      proj.loading.boot[[f]] <- lapply(X =  assay.list,
                                       FUN = function(assay){
                                         boot.pca.multi[[assay]]@feature.loadings 
                                       })
      if(verbose){
        setTxtProgressBar(pb, f)
      }
    }
    if(verbose){
      close(pb)
    }
  } else{
    proj.embedding.boot[[1]] <- lapply(X =  assay.list,
                                       FUN = function(assay) {
                                         object[[reduction.list[[assay]]]]@cell.embeddings[ , dims.list[[assay]]]
                                       })
  }
  modality.weight <- ModalityWeights(object = object, 
                                     bootstrap.embedding.list =  proj.embedding.boot,
                                     assay.list = assay.list,
                                     distance = distance,
                                     angular.nn = angular.nn,
                                     kernel = kernel,
                                     kernel.power = kernel.power, 
                                     reduction.list = reduction.list,
                                     dims.list = dims.list, 
                                     knn.list = knn.list,
                                     mean.type = mean.type,
                                     nn.metric = nn.metric, 
                                     score.c = score.c,
                                     stdev.weight = stdev.weight, 
                                     max.modality_score = max.modality_score,
                                     verbose = verbose)
  return( modality.weight )
}


snn_nn <- function(snn.graph, k.nn, far.nn = TRUE){
  if(far.nn){
    direction <- 1
  } else{
    direction <- (-1)
  }
  nn.idx.snn <- t(apply(snn.graph, 
                        MARGIN = 1,
                        FUN = function(x){
                          x.nonzero.idx <- which(x !=0 )
                          x.nonzero <- x[ x.nonzero.idx ]
                          nn.idx <-  x.nonzero.idx[which(rank(x.nonzero * direction,  ties.method = "first") <= (k.nn) )]
                          return(nn.idx)
                        } ))
  return(nn.idx.snn)
  
}
 


FindModalityWeights.kernel <- function(object, 
                                       query = NULL, 
                                       reduction.list, 
                                       dims.list, 
                                       sd.scale = 0.75, 
                                       cross.contant.list = NULL, 
                                       snn.far.nn = FALSE, 
                                       k.nn = 20, 
                                       s.nn = NULL, 
                                       sigma.idx = NULL,
                                       l2.norm = FALSE, 
                                       KL.divergence = FALSE, 
                                       smooth = FALSE
                                  ){
  if(is.null(s.nn)){
    s.nn <- k.nn
  }
  if(is.null(sigma.idx)){
    sigma.idx <- 20
  }
  if(is.null(cross.contant.list)){
    cross.contant.list <- list(1e-4, 1e-4)
  }
  reduction.set <- unlist(reduction.list)
  names(reduction.list) <- names(dims.list)  <- names(cross.contant.list) <- reduction.set

  embeddings.list <- lapply( X = reduction.list, 
                             FUN = function(r) Embeddings(object = object, reduction = r)[, dims.list[[r]]  ])

  if(l2.norm){
    embeddings.list.norm <- lapply( X = embeddings.list,
                                          FUN = function(embeddings) L2Norm(embeddings)) 
  } else{
   embeddings.list.norm <- embeddings.list
  }
  
  if(is.null(query)){
    query.embeddings.list.norm <- embeddings.list.norm
    query <- object
  } else{ 
    if(snn.far.nn){
      message("query do not support to use snn to find distant neighbors")
    }
    query.embeddings.list <- lapply( X = reduction.list, 
                               FUN = function(r) Embeddings(object = query, reduction = r)[, dims.list[[r]]  ])
    if(l2.norm){
      query.embeddings.list.norm <- lapply( X = query.embeddings.list,
                                      FUN = function(embeddings) L2Norm(embeddings)) 
    } else{
      query.embeddings.list.norm <- query.embeddings.list
    }
    }
  
  
  nn.list <- lapply(X = reduction.list, 
                    FUN = function(r){
                     nn.r <-  NNHelper(data = embeddings.list.norm[[r]],
                                       query = query.embeddings.list.norm[[r]],
                               k = max(k.nn, sigma.idx, s.nn), 
                               method = "annoy", 
                               metric = "euclidean")
                     rownames(nn.r$nn.idx) <- Cells(query)
                     return(nn.r)
                    })
  sigma.nn.list <- nn.list

  if( sigma.idx > k.nn || s.nn > k.nn){
    nn.list <- lapply(X = nn.list, 
                      FUN = function(nn){
                        nn$nn.idx <-  nn$nn.idx[, 1:k.nn]
                        nn$nn.dists <-  nn$nn.dists[, 1:k.nn]
                        return(nn)
                      })
  }

  within_impute <- list()
  cross_impute <- list()
  for (r in reduction.set ){
    reduction.norm <- paste0(r, ".norm")
    object[[ reduction.norm ]] <- CreateDimReducObject(embeddings = embeddings.list.norm[[r]],
                                                         key = paste0("norm", object[[r]]@key), 
                                                         assay = object[[r]]@assay.used )
    within_impute[[r]] <- PredictAssay(object = object, 
                                       nn.idx =  nn.list[[r]]$nn.idx,
                                       reduction = reduction.norm,
                                       dims = 1:ncol(embeddings.list.norm[[r]]), 
                                       verbose = F,
                                       cells = Cells(query),
                                       return.assay = F )
    
    cross_impute[[r]]<- PredictAssay(object = object,
                                     nn.idx = nn.list[[setdiff(reduction.set, r ) ]]$nn.idx,
                                     reduction = reduction.norm, 
                                     dims = 1:ncol(embeddings.list.norm[[r]]), 
                                     verbose = F,
                                     cells = Cells(query),
                                     return.assay = F )
  }
  
  within_impute_dist <- lapply( X = reduction.list, 
                                FUN = function(r){
                                  sqrt(rowSums((query.embeddings.list.norm[[r]] - t(within_impute[[r]]))**2))
                                } )
  cross_impute_dist <- lapply( X = reduction.list, 
                               FUN = function(r){
                                 sqrt(rowSums((query.embeddings.list.norm[[r]] - t(cross_impute[[r]]))**2))
                               } )

 if(snn.far.nn){
   snn.graph.list <- lapply(X = sigma.nn.list,
                            FUN = function(nn){
                              snn.matrix <- ComputeSNN(
                                nn_ranked =  nn$nn.idx[, 1:s.nn],
                                prune = 0
                              )
                     colnames(snn.matrix) <- rownames( snn.matrix) <- Cells(object)
                     return(snn.matrix)
                            })
 
   snn.far.nn.list <- lapply(X = snn.graph.list,
                             FUN = function(snn) {
                               snn_nn(snn.graph = snn,  k.nn = k.nn, far.nn = TRUE)
                             }  )

   distant_nn_dist <- lapply( X = reduction.list, 
                              FUN = function(r){
                                t(sapply(X = 1:ncol(object), 
                                         FUN = function(x){
                                           rdist::cdist( X = embeddings.list.norm[[r]][ x, , drop = F],  
                                                  Y = embeddings.list.norm[[r]][ snn.far.nn.list[[r]][x, ], ],
                                                  metric = "euclidean" )
                                         }))
                                
                              })
     
  modality_sd.list <- lapply( X = distant_nn_dist , function(d)  rowMeans(d)*sd.scale)
   } else{
  modality_sd.list <-  lapply( X = sigma.nn.list , function(sigma.nn)  sigma.nn$nn.dists[,sigma.idx])
 }
  within_impute_kernel <- lapply( X = reduction.list,
                                  FUN = function(r) {
                                    exp(-1*( within_impute_dist[[r]]/modality_sd.list[[r]] )**2) 
                                  })
  
  cross_impute_kernel<- lapply( X = reduction.list,
                                FUN = function(r) {
                                  exp(-1*( cross_impute_dist[[r]]/modality_sd.list[[r]] )**2) 
                                })
  
  params <- list( reduction.list,
                   dims.list,
                  l2.norm,
                  sigma.idx,
                  snn.far.nn ,
                  modality_sd.list)
  
  names(params) <- c("reduction.list","dims.list","l2.norm", "sigma.idx", "snn.far.nn","sigma.list")

  
  modality_score <-  lapply( X = reduction.list,
                             FUN = function(r) {
                               score = within_impute_kernel[[r]] / 
                                 ( cross_impute_kernel[[r]] + cross.contant.list[[r]] )
                               score = MinMax(score, min = 0, max = 200)
                             })
 
  if(smooth){
    modality_score <- lapply( X = reduction.list, 
                             FUN = function(r){
                               apply( X = nn.list[[r]]$nn.idx,
                                      MARGIN = 1, 
                                      FUN = function(nn) mean( modality_score[[r]][ nn[-1]])) 
                             } )
  }

  if(KL.divergence){
    modality1.weight <-  within_impute_kernel[[1]]*log(modality_score[[1]]) / ( within_impute_kernel[[1]]*log(modality_score[[1]]) +  within_impute_kernel[[2]]*log(modality_score[[2]]))
    modality1.weight <- MinMax(modality1.weight, min = 0, max = 1)
  }else{
    modality1.weight <- exp(modality_score[[1]])/(exp(modality_score[[1]]) + exp(modality_score[[2]]))
  }
  
  score.mat<- cbind(Reduce(cbind, within_impute_dist), 
        Reduce(cbind, cross_impute_dist), 
        Reduce(cbind, within_impute_kernel), 
        Reduce(cbind, cross_impute_kernel), 
        Reduce(cbind, modality_score))
  
  colnames(score.mat) <- c( "modality1_nn1", "modality2_nn2", 
                            "modality1_nn2",  "modality2_nn1", 
                            "modality1_nn1_kernel",  "modality2_nn2_kernel",
                           "modality1_nn2_kernel",  "modality2_nn1_kernel",
                            "modality1_score", "modality2_score")
  score.mat <- as.data.frame(score.mat)
 
  weight.list <- list(modality1.weight, params, score.mat )
  names(weight.list) <- c("first.modality.weight", "params", "score.matrix")
  return(weight.list)
}



#' Dimensional reduction for FindModalityWeights.Multi
#'
#' @param object A Seurat object
#' @param assay Assay name added in the reduction assay
#' @param type Data type, such as RNA, ATAC
#' @param npcs The number of PC calculated
#' @param verbose Display messages
#'
#' @return Returns a reduction assay

RunSVD.bootstrapping <- function(object, 
                                 boot.object,
                                 assay,
                                 reduction, 
                                 dims,
                                 type = "RNA", 
                                 npcs = 50, 
                                 slot, 
                                 verbose = FALSE){
  if(type =="ATAC"){
    npcs <- min(npcs, nrow(x = boot.object) - 1)
    svd.results <- irlba(A = t(x = boot.object), nv = npcs)
    feature.loadings <- svd.results$v[, dims]
    sdev <- svd.results$d[dims]
    boot.feature <- rownames(feature.loadings) <- rownames(boot.object)
    cell.embeddings <- t( GetAssayData(object = object, 
                                       assay = assay,
                                       slot = slot )[boot.feature, ]) %*% 
      feature.loadings[ boot.feature, ] %*% diag(1/sdev)
    embed.mean <- apply(X =  svd.results$u[ ,dims], MARGIN = 2, FUN = mean)
    embed.sd <- apply(X =  svd.results$u[ ,dims], MARGIN = 2, FUN = sd)
    norm.cell.embeddings <- t((t(cell.embeddings) - embed.mean) / embed.sd)
    rownames(x = feature.loadings) <- rownames(x = boot.object)
    colnames(x = norm.cell.embeddings) <- colnames(x = feature.loadings) <- paste0("LSI_", dims)
    reduction.data <- CreateDimReducObject(
      embeddings = as.matrix(norm.cell.embeddings),
      loadings = feature.loadings,
      assay = assay,
      stdev = sdev,
      key = "LSI_"
    )
  } else{
    boot.object <- ScaleData(boot.object, do.scale = FALSE, do.center = TRUE, verbose = FALSE ) 
    boot.reduction.data <- suppressWarnings( RunPCA(boot.object, npcs =  npcs ,assay = assay, verbose = verbose))
    boot.feature <- rownames(boot.reduction.data@feature.loadings)
    cell.embeddings <- t( GetAssayData(object = object, 
                                       assay = assay,
                                       slot = slot )[boot.feature, ] ) %*% 
      boot.reduction.data@feature.loadings[ boot.feature, dims] 
    reduction.data <- CreateDimReducObject(
      embeddings = cell.embeddings,
      loadings = boot.reduction.data@feature.loadings[,dims],
      assay = assay,
      stdev = boot.reduction.data@stdev[dims],
      key = "PC_"
    )
  }
  return(reduction.data)
}

#' Calculate cosine times projection similarity 
#' given boostatrping embeddings
#'
#' @param object A Seurat object
#' @param bootstrap.embedding.list A list of bootstaping cell embeddings
#' @param assay.list A list of names of assays
#' @param reduction.list A list of name of dimension reduction 
#' @param dims.list A list of number of dimensions to use
#' @param knn.list A list of number of nearest neighbors to use
#' @param mean.type the mean method to average neighbors
#' @param nn.metric distance metric for finding neighbors
#' @param clip.range clip range of similarity
#' @param score.c a list of the minimal cross modality prediction similarity
#' @param max.modality_score the maximal modality score
#' @param verbose Display messages
#' 
#' @importFrom pracma dot
#' @return Returns a vector of angular distance to x
#' 
ModalityWeights <- function(object,
                            bootstrap.embedding.list,
                            assay.list,
                            reduction.list, 
                            dims.list,
                            angular.nn = FALSE, 
                            distance = c("proj_angular",  "angular", "projection"  ,"euclidean")[1],
                            kernel = FALSE,
                            kernel.power = 2, 
                            knn.list = NULL, 
                            mean.type = NULL,
                            nn.metric = "cosine",
                            score.c = list(0.1, 0.1), 
                            stdev.weight = TRUE,
                            max.modality_score = 700, 
                            clip.range = c(0.01, 1), 
                            verbose = TRUE){
  feature.multi <- lapply( X = assay.list, FUN = function(assay) rownames(object[[reduction.list[[assay]]]]@feature.loadings) )
  assay.comb <-  as.data.frame(gtools::permutations(n = length(assay.list), r = 2,v = unlist(assay.list),  repeats.allowed = T))
  colnames(assay.comb) <- c("assay", "nn")
  assay.comb$assay <- as.character(assay.comb$assay)
  assay.comb$nn <- as.character(assay.comb$nn)
  
  projcos.multi.boot <- list()
  embedding.multi <- lapply( X = assay.list,
                             FUN = function(assay){
                               t(object[[reduction.list[[assay]]]]@cell.embeddings[ , dims.list[[assay]]] )
                             } ) 
  embedding.dot.product.multi <- lapply( X = embedding.multi, 
                                         FUN = function(embedding){
                                           colSums(matrixcalc::hadamard.prod(embedding, embedding  ))  
                                         } )
  if(verbose){
    message("Predicting modality weights")
    pb <- txtProgressBar(min = 0, max = length(  length(bootstrap.embedding.list[[1]])), style = 3)
  }
  for (b in 1:length(bootstrap.embedding.list)){
    bs_nn.multi <- lapply(X = assay.list,
                          FUN = function(assay){
                            NNHelper( data = bootstrap.embedding.list[[b]][[assay]],
                                      k = knn.list[[assay]],
                                      method = "annoy", 
                                      metric = nn.metric) 
                          })
    bs_nn.multi <- lapply(X = bs_nn.multi, FUN = function(nn){ rownames(nn$nn.idx)<- Cells(object) ; nn} )
    projcos.multi.boot[[b]] <- lapply( X = 1:nrow(assay.comb), 
                                       FUN =  function(c){
                                         projcosine(object = object, 
                                                    nn.idx = bs_nn.multi[[ assay.comb[c,"nn"] ]]$nn.idx, 
                                                    reduction.list = reduction.list[[assay.comb[c,"assay"]]], 
                                                    dims.list = dims.list[[ assay.comb[c,"assay"] ]], 
                                                    embedding.dot.product = embedding.dot.product.multi[[ assay.comb[c,"assay"]]],
                                                    embedding.list = embedding.multi[[ assay.comb[c,"assay"] ]]  )
                                       })
    if(verbose){
      setTxtProgressBar(pb, b)
    }
  }
  if(verbose){
    close(pb)
  }
  proj.multi.mean <- lapply(X = 1:nrow(assay.comb), 
                            FUN = function(c){
                              rowMeans( 
                                sapply(X = 1:length(projcos.multi.boot), 
                                       FUN = function(b) projcos.multi.boot[[b]][[c]]$proj.similarity[[1]] )
                              )
                            }  )
  proj.multi.mean <- lapply(X = proj.multi.mean, 
                            FUN = function(proj){ 
                              MinMax( data = proj, min = min(clip.range), max = max(clip.range) ) 
                            })
  cosine.multi.mean <- lapply(X = 1:nrow(assay.comb), 
                              FUN = function(c){
                                rowMeans( 
                                  sapply(X = 1:length(projcos.multi.boot), 
                                         FUN = function(b) projcos.multi.boot[[b]][[c]]$cosine.similarity[[1]] ) 
                                )
                              })
  cosine.multi.mean <- lapply(X = cosine.multi.mean, 
                              FUN = function(cosine){
                                MinMax( data = cosine, min = min(clip.range), max = max(clip.range) ) 
                              })
   proj_cosine <- lapply(X = 1:nrow(assay.comb), 
                         FUN = function(c)  sqrt(cosine.multi.mean[[c]] * proj.multi.mean[[c]])   )
   nn_dist <- lapply(X = 1:nrow(assay.comb), 
                     FUN = function(c){
                       projangular_nn_dist(   bs_nn.multi[[ assay.comb$nn[c] ]]$nn.idx,  
                                          bootstrap.embedding.list[[1]][[ assay.comb$assay[c]]])
                       
                     })
   if(angular.nn){
     angular.nn.mean <- lapply(X = nn_dist ,
                               FUN = function(dist){ 
                                 sapply( X = dist$angular, 
                                         FUN = function(angular) mean(angular)**2)
                               })
     assay.comb.dist <- angular.nn.mean
   } else{
     
     
     eu.multi.mean <- lapply(X = 1:nrow(assay.comb), 
                             FUN = function(c){
                               rowMeans( 
                                 sapply(X = 1:length(projcos.multi.boot), 
                                        FUN = function(b) projcos.multi.boot[[b]][[c]]$euclidean.distance[[1]] )
                               )
                             })
     nn_dist.eu.sd <-  lapply(X = assay.list, 
                              FUN = function(assay){
                                sapply( X= nn_dist[[which(assay.comb$assay == assay & assay.comb$nn == assay )]]$euclidean, 
                                        function(x) sort(x)[5])
                              })
     
     all_impute_dist <- list(cosine.multi.mean,proj.multi.mean, proj_cosine ,eu.multi.mean)
     names(all_impute_dist) <- names(nn_dist[[1]])
     
     all_dist_sd <-  lapply(X = names(all_impute_dist),
                            FUN = function(d){ 
                              lapply(X = assay.list, 
                                     FUN = function(assay){
                                       sapply( X = nn_dist[[which(assay.comb$assay == assay & assay.comb$nn == assay )]][[d]], 
                                               function(x) sort(x)[5])
                                     })
                            })
     names(all_dist_sd) <-  names(all_impute_dist)
     assay.comb.dist <- all_impute_dist[[distance]]
     
     if(distance == "euclidean"){
       assay.comb.dist <- lapply(X = 1:nrow(assay.comb), 
                                 FUN = function(c){
                                   exp(-0.5*(all_impute_dist[[distance]][[c]] / 
                                               (all_dist_sd[[distance]][[ assay.comb[c,"assay"] ]] ) )**kernel.power)}
       ) 
     } else{
       if(kernel){
         assay.comb.dist <- lapply(X = 1:nrow(assay.comb), 
                                   FUN = function(c){
                                     exp(-0.5*( (1 - all_impute_dist[[distance]][[c]]) / 
                                                  ( 1 - all_dist_sd[[distance]][[ assay.comb[c,"assay"] ]] ) )**kernel.power) 
                                   }) 
         
       }
       
     }
     
   }
   
if(stdev.weight){
    modality_score <- lapply(X = assay.list,
                             FUN =  function(assay){
                               (sd(object[[reduction.list[[assay]]]]@stdev) / mean( object[[reduction.list[[assay]]]]@stdev))*
                                 (assay.comb.dist[[ which(assay.comb$assay == assay & assay.comb$nn == assay ) ]] ) / 
                                 (assay.comb.dist[[ which(assay.comb$assay == assay & assay.comb$nn != assay ) ]] + score.c[[assay]])
                             }  )
  } else{
    modality_score <- lapply(X = assay.list, 
                             FUN = function(assay){
                               (assay.comb.dist[[ which(assay.comb$assay == assay & assay.comb$nn == assay ) ]] ) / 
                                 (assay.comb.dist[[ which(assay.comb$assay == assay & assay.comb$nn != assay ) ]] + 
                                    score.c[[assay]]   )
                             })
  }
  modality_score <- lapply(X = modality_score,
                           FUN = function(score) {
                             MinMax(score, min = 0, max = max.modality_score)
                           })      

  first.modality.weight <- exp(modality_score[[1]]) / (exp(modality_score[[1]]) + exp(modality_score[[2]]))
  weight.list <- list(  assay.comb.dist,  modality_score , first.modality.weight, assay.comb)
  names(weight.list) <-c(  "assay.comb.dist",  "modality_score" , "first.modality.weight", "assay.comb")
  return(weight.list)
}



#' Calculate cosine times projection similarity given the knn indices
#'
#' @param object A Seurat object
#' @param nn.idx k near neighbour indices. A cells x k matrix.
#' @param assay.rna Name of the RNA assay
#' @param assay.adt Name of the ADT assay
#' @param reduction.rna Name of dimension reduction for the RNA assay
#' @param reduction.adt Name of dimension reduction for the ADT assay
#' @param dims.rna Number of dimensions to use for the RNA assay
#' @param dims.adt Number of dimensions to use for the ADT assay
#' @param verbose Display messages
#' 
#' @importFrom pracma dot
#' @return Returns a list of similarities
#' 
projcosine <- function(object, 
                       nn.idx,
                       embedding.list = NULL,
                       embedding.dot.product = NULL, 
                       reduction.list, 
                       dims.list,
                       mean.type = NULL,
                       verbose = FALSE
){
  if(is.null(embedding.list)){
    embedding.list <- lapply( X = 1:length(reduction.list),
                              FUN = function(r){
                                t(Embeddings(object = object, reduction = reduction.list[[r]])[, dims.list[[r]]] )
                              }
    )
    embedding.dot.product <- lapply( X = embedding.list, 
                                     FUN = function(embedding){
                                       colSums(matrixcalc::hadamard.prod(embedding, embedding))
                                     } )
  }
  if(!is.list(embedding.list)){
    embedding.list <- list(embedding.list)
  }
  if(!is.list(embedding.dot.product)){
    embedding.dot.product <- list(embedding.dot.product)
  }
  if(!is.list(reduction.list)){
    reduction.list <- list(reduction.list)
  }
  if(!is.list(dims.list)){
    dims.list <- list(dims.list)
  }
  
  predict.embedding.list <- lapply(1:length(reduction.list), 
                                   FUN = function(r){
                                     PredictAssay(object = object, 
                                                  reduction = reduction.list[[r]],
                                                  dims =  dims.list[[r]],
                                                  mean.type = mean.type,
                                                  nn.idx = nn.idx,
                                                  return.assay = F,
                                                  verbose = verbose )
                                   }  )
  eu.distance <- lapply(X = 1:length(reduction.list),
                         FUN = function(r){
                           sapply(1:ncol(embedding.list[[r]]),
                                  function(c) cdist( X = t(predict.embedding.list[[r]][,c]), 
                                                     Y = t(embedding.list[[r]][,c]), 
                                                     metric = "euclidean"))
                         })
  proj.similarity <- lapply(X = 1:length(reduction.list),
                            FUN = function(r){
                              colSums(matrixcalc::hadamard.prod(predict.embedding.list[[r]], embedding.list[[r]]  )) / 
                                embedding.dot.product[[r]]  
                            })
  cosine.similarity <- lapply(X = 1:length(reduction.list), 
                              FUN = function(r){
                                proj.similarity[[r]] * 
                                  sqrt(embedding.dot.product[[r]] ) / 
                                  sqrt(colSums(
                                    matrixcalc::hadamard.prod(predict.embedding.list[[r]],predict.embedding.list[[r]])))  
                              } )
  cosine.similarity <- lapply(X = cosine.similarity, 
                              FUN = function(cosine) {
                                1 - 2*acos(cosine)/pi
                              } )
  loss.list <- list(proj.similarity, cosine.similarity,  eu.distance, predict.embedding.list )
  names(loss.list) <-c("proj.similarity", "cosine.similarity", "euclidean.distance", "predicted.embedding")
  return(loss.list)
}


#' Joint linear discriminant analysis
#' It is used to interprete the joint clustering results
#' to deconvulte the combination of different modalities
#' @param object A Seurat object
#' @param reduction.list A list of name of dimention reduction
#' @param dims.list A list of number of dimention 
#' @param labels Observation labels for LDA
#' @param lda.weight.name The name of modality weights stored
#' @param new.assay The name of assay used for multiple reduction 
#' cell embeddings
#' @param new.reduction.name name of LDA redunction name, lda by default
#' @param new.reduction.key dimensional reduction key, specifies the string before
#' the number for the dimension names. LDA_ by default
#' 
#' @export
#'
JointLDA <- function(object, 
                     reduction.list, 
                     dims.list, 
                     labels,
                     lda.weight.name = "RNA.lda.weight", 
                     new.assay= "Reduc", 
                     new.reduction.name = "lda",
                     new.reduction.key = "LDA_"
){
  
  joint.embedding <- cbind(object[[reduction.list[[1]]]]@cell.embeddings[, dims.list[[1]]] , 
                           object[[reduction.list[[2]]]]@cell.embeddings[, dims.list[[2]]] )
  object[[new.assay]] <- CreateAssayObject(data = t(joint.embedding))
  DefaultAssay(object) <- new.assay
  object <- ScaleData(object, assay = new.assay, do.center = T, do.scale = F)
  object <- RunLDA.Seurat(object = object, 
                          assay = new.assay, 
                          labels = as.character(labels), 
                          features = rownames(object[[new.assay]]),
                          reduction.name = new.reduction.name,
                          reduction.key =  new.reduction.key )
  
  #calculate modality weights in LDA
  lda.weight <- sapply(X = 1:ncol(object[[new.reduction.name]] ), 
                       FUN = function(x){
                         mean(abs(object[[new.reduction.name]]@feature.loadings[dims.list[[1]] ,x])) / 
                           (mean(abs(object[[new.reduction.name]]@feature.loadings[1 : length(dims.list[[1]]) ,x])) + 
                              mean(abs(object[[new.reduction.name]]@feature.loadings[(length(dims.list[[1]])+1):(length(dims.list[[1]]) + length(dims.list[[2]]) ),x]))
                           )  
                       })
  
  object[[new.reduction.name]]@misc[[lda.weight.name]] <- lda.weight
  return(object)
}



FilterADT <- function(object, 
                      assay = "ADT", 
                      dims, 
                      reduction = "pca",
                      slot = "data", 
                      overlap.cutoff = 10, 
                      k.nn = 30, 
                      positive.probs = 0.9,
                      negative.probs = 0.1
){
  
  count.overlap.list <- list()
  adt.cutoff.list <- list()
  adt.cutoff.list.low <- list()
  adt.data <- GetAssayData(object = object, assay = assay, slot = slot )
  feature.adt.set <- rownames(adt.data)
  
  for (f in 1:length(feature.adt.set)){
    
    feature.adt <- feature.adt.set[f]
    
    adt.cutoff <- quantile(x = adt.data[feature.adt, ], probs = positive.probs)
    
    
    adt.cutoff.list[[f]] <- adt.cutoff
    adt.cutoff.list.low[[f]] <-  quantile(x = adt.data[feature.adt, ], probs = negative.probs)
    if( adt.cutoff.list.low[[f]] == adt.cutoff.list[[f]] ){
      adt.cutoff.list[[f]] <- quantile(x = adt.data[feature.adt, ], probs = 1)
    }
    
    positive.idx <- which(adt.data[feature.adt, ] >= adt.cutoff)
    positive.cell <- Cells(object)[positive.idx]
    
    postive.nn <- NNHelper(data = object[[reduction]]@cell.embeddings[, dims], 
                           query = object[[reduction]]@cell.embeddings[positive.cell, dims], 
                           k = k.nn, 
                           method = "annoy"   )
    
    overlap.pos <- function(x, y ) length(intersect(x, y))
    positive.nn.cover <- apply(X = postive.nn$nn.idx[,-1], MARGIN = 1 , y = positive.idx, FUN =  overlap.pos)
    
    
    count.overlap.list[[f]] <- sum(positive.nn.cover > overlap.cutoff)
  }
  
  names(count.overlap.list) <- names(adt.cutoff.list) <- names(adt.cutoff.list.low) <- feature.adt.set
  count.overlap.list <- unlist(count.overlap.list)
  adt.cutoff.list <- unlist(adt.cutoff.list)
  adt.cutoff.list.low <- unlist(adt.cutoff.list.low)
  
  #count.overlap.list <- unlist(count.overlap.list)
  return( list(count.overlap.list, adt.cutoff.list, adt.cutoff.list.low))
}



#' Convert knn into snn
#'
#' @param object A Seurat object
#' @param wknn.use knn graph
#' @param prune.SNN Sets the cutoff for acceptable Jaccard index when
#' computing the neighborhood overlap for the SNN construction.
#'
#' @importFrom Matrix tcrossprod
#' @return Returns a snn matrix
#' 
knn_snn_graph <- function(object,
                          knn.matrix, 
                          prune.SNN = 1/15){
  knn.matrix <- knn.matrix %*% t(knn.matrix)
  knn.matrix <- knn.matrix/diag(knn.matrix)
  edge.summary <- Matrix::summary(knn.matrix)
  edge.summary <- edge.summary[edge.summary$x > prune.SNN, ]

  snn.graph <- sparseMatrix(i = edge.summary$i,
                               j = edge.summary$j,
                               x = edge.summary$x,
                               dims = c(nrow(knn.matrix), ncol(knn.matrix))
  )
  rownames(snn.graph) <- Cells(object)
  colnames(snn.graph) <- Cells(object)
  return(snn.graph)
}


#' Calculate angular distance
#'
#' @param x a vector of number
#' @param y a vector of number, or a matrix
#' @importFrom rdist cdist
#' @return Returns a vector of angular distance to x
#' 
angular <- function(x,y){
  if( is.null(ncol(x)) & is.null( ncol(y))){
    return(  rdist::cdist(t(x),t(y), metric = "angular")*2/pi )
  } else{
    dist.vector <- sapply(1:nrow(y), function(c)  rdist::cdist(t(x),t(y[c,]), metric = "angular")*2/pi )
    return(dist.vector)
  }
  
}


